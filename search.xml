<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[VMware安装MacOS]]></title>
      <url>%2F2017%2F01%2F05%2Fmac%2F</url>
      <content type="text"><![CDATA[准备工作：VMware12OX 10.12 Sierra镜像cdrUnlocker208–VMware上的Mac补丁（bios intelVT开启）工具地址：链接：http://pan.baidu.com/s/1nv0iiZB 密码：f1twVMware Workstation 12序列号: 5A02H-AU243-TZJ49-GTC7K-3C61N 安装过程 安装VMware 安装mac补丁解压unlocker208，右键win-install.cmd以管理员运行，然后等待cmd窗口自己关闭，unlocker208安装完成 安装VMware里面的mac上述工作准备完成后新建虚拟机里面就会多出Apple Mac OS X(M)选项，安转即可 不可恢复错误解决问题解决 mac安转完成打开出现提示不可恢复错误，此时进入虚拟机安装目录用记事本类的编辑器，编辑macOS 10.12.vmx加入以下代码：smc.version = &quot;0&quot;完工]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过路由器自定义host实现iOS 10科学上网]]></title>
      <url>%2F2017%2F01%2F01%2Fhost%2F</url>
      <content type="text"><![CDATA[ios 10到现在也没越狱，虽然商店有vpn，但是有广告，不方便，也不够安全，既然是连接到路由的，那么更改路由host来实现科学上网，就爽了 过程我的路由器是newifi现在刷的是Pandorabox是基于openwrt的路由系统，系统集成的dhcp/dns服务可以自定义host。用winscp连接路由上传我们的host文件到路由，然后在路由设置里的额外的hosts文件打钩，填入我们刚刚的host。然后ssh连接路由输入命令/etc/init.d/dnsmasq restartPs：如果是电脑本地的dns不能使用自定义的，需要改为路由器的dns，其实直接改为自动获取dns就行了。 hosthost可以在github找到，或者在这个持续更新的帖子里http://htcui.com/4938.html下载就行]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python爬取微博]]></title>
      <url>%2F2016%2F12%2F29%2F1%2F</url>
      <content type="text"><![CDATA[渣浪微博算是用的比较多了，但是一条一条的翻看太过于麻烦，无奈爬取之。 环境：win7+python2.7； python模块：request；bs4；beautifulsoup4；lxml； chrome浏览器 实现过程：获取cookie：打开chrome然后Ctrl+Shift+I调出开发者工具，点击tooggle device toolbar(Ctrl+shift+M)然后在Responsive选择合适的设备，我这里选择的是nexus5X，接下来打开weibo.com这时候应该看到的就是手机端网页的微博登录页面。在开发者工具中选中Network–Preserve log，然后登录微博。在开发者工具中找到有关m.weibo.cn的复制自己的cookie获取你想爬取微博的user_id：就是你要爬取的微博的主页网址中weibo.com/u/后面的数字部分开始利用python脚本爬取这是python程序的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#-*-coding:utf8-*-import reimport stringimport sysimport osimport urllibimport urllib2from bs4 import BeautifulSoupimport requestsfrom lxml import etreereload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)if(len(sys.argv)&gt;=2): user_id = (int)(sys.argv[1])else: user_id = (int)(raw_input(u&quot;请输入user_id: &quot;))cookie = &#123;&quot;Cookie&quot;: &quot;#your cookie&quot;&#125;url = &apos;http://weibo.cn/u/%d?filter=1&amp;page=1&apos;%user_idhtml = requests.get(url, cookies = cookie).contentselector = etree.HTML(html)pageNum = (int)(selector.xpath(&apos;//input[@name=&quot;mp&quot;]&apos;)[0].attrib[&apos;value&apos;])result = &quot;&quot; urllist_set = set()word_count = 1image_count = 1print u&apos;爬虫准备就绪...&apos;for page in range(1,pageNum+1): #获取lxml页面 url = &apos;http://weibo.cn/u/%d?filter=1&amp;page=%d&apos;%(user_id,page) lxml = requests.get(url, cookies = cookie).content #文字爬取 selector = etree.HTML(lxml) content = selector.xpath(&apos;//span[@class=&quot;ctt&quot;]&apos;) for each in content: text = each.xpath(&apos;string(.)&apos;) if word_count&gt;=4: text = &quot;%d :&quot;%(word_count-3) +text+&quot;\n\n&quot; else : text = text+&quot;\n\n&quot; result = result + text word_count += 1 #图片爬取 soup = BeautifulSoup(lxml, &quot;lxml&quot;) urllist = soup.find_all(&apos;a&apos;,href=re.compile(r&apos;^http://weibo.cn/mblog/oripic&apos;,re.I)) first = 0 for imgurl in urllist: urllist_set.add(requests.get(imgurl[&apos;href&apos;], cookies = cookie).url) image_count +=1fo = open(&quot;/Users/Personals/%s&quot;%user_id, &quot;wb&quot;)fo.write(result)word_path=os.getcwd()+&apos;/%d&apos;%user_idprint u&apos;文字微博爬取完毕&apos;link = &quot;&quot;fo2 = open(&quot;/Users/Personals/%s_imageurls&quot;%user_id, &quot;wb&quot;)for eachlink in urllist_set: link = link + eachlink +&quot;\n&quot;fo2.write(link)print u&apos;图片链接爬取完毕&apos;if not urllist_set: print u&apos;该页面中不存在图片&apos;else: #下载图片,保存在当前目录的pythonimg文件夹下 image_path=os.getcwd()+&apos;/weibo_image&apos; if os.path.exists(image_path) is False: os.mkdir(image_path) x=1 for imgurl in urllist_set: temp= image_path + &apos;/%s.jpg&apos; % x print u&apos;正在下载第%s张图片&apos; % x try: urllib.urlretrieve(urllib2.urlopen(imgurl).geturl(),temp) except: print u&quot;该图片下载失败:%s&quot;%imgurl x+=1print u&apos;原创微博爬取完毕，共%d条，保存路径%s&apos;%(word_count-4,word_path)print u&apos;微博图片爬取完毕，共%d张，保存路径%s&apos;%(image_count-1,image_path) 可以将上面的代码保存为wb.py然后在cmd里面运行1python wb.py 大功告成 2016/12/29 星期四 3:16:40 补充：python模块的安装可以用easy_install的命令安装 例如1easy_install lxml python模块pip安装：http://jingyan.baidu.com/article/e73e26c0d94e0524adb6a7ff.html用Python写一个简单的微博爬虫: http://www.jianshu.com/p/7c5a4d7545caMicrosoft Visual C++ Compiler for Python 2.7:http://www.microsoft.com/en-us/download/details.aspx?id=44266]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
