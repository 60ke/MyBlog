<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[python爬取微博]]></title>
      <url>%2F2016%2F12%2F29%2F1%2F</url>
      <content type="text"><![CDATA[渣浪微博算是用的比较多了，但是一条一条的翻看太过于麻烦，无奈爬取之。 环境：win7+python2.7； python模块：request；bs4；beautifulsoup4；lxml； chrome浏览器 实现过程： 获取cookie：打开chrome然后Ctrl+Shift+I调出开发者工具，点击tooggle device toolbar(Ctrl+shift+M)然后在Responsive选择合适的设备，我这里选择的是nexus5X，接下来打开weibo.com这时候应该看到的就是手机端网页的微博登录页面。在开发者工具中选中Network–Preserve log，然后登录微博。在开发者工具中找到有关m.weibo.cn的复制自己的cookie 获取你想爬取微博的user_id：就是你要爬取的微博的主页网址中weibo.com/u/后面的数字部分 开始利用python脚本爬取这是python程序的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#-*-coding:utf8-*-import reimport stringimport sysimport osimport urllibimport urllib2from bs4 import BeautifulSoupimport requestsfrom lxml import etreereload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)if(len(sys.argv)&gt;=2): user_id = (int)(sys.argv[1])else: user_id = (int)(raw_input(u&quot;请输入user_id: &quot;))cookie = &#123;&quot;Cookie&quot;: &quot;#your cookie&quot;&#125;url = &apos;http://weibo.cn/u/%d?filter=1&amp;page=1&apos;%user_idhtml = requests.get(url, cookies = cookie).contentselector = etree.HTML(html)pageNum = (int)(selector.xpath(&apos;//input[@name=&quot;mp&quot;]&apos;)[0].attrib[&apos;value&apos;])result = &quot;&quot; urllist_set = set()word_count = 1image_count = 1print u&apos;爬虫准备就绪...&apos;for page in range(1,pageNum+1): #获取lxml页面 url = &apos;http://weibo.cn/u/%d?filter=1&amp;page=%d&apos;%(user_id,page) lxml = requests.get(url, cookies = cookie).content #文字爬取 selector = etree.HTML(lxml) content = selector.xpath(&apos;//span[@class=&quot;ctt&quot;]&apos;) for each in content: text = each.xpath(&apos;string(.)&apos;) if word_count&gt;=4: text = &quot;%d :&quot;%(word_count-3) +text+&quot;\n\n&quot; else : text = text+&quot;\n\n&quot; result = result + text word_count += 1 #图片爬取 soup = BeautifulSoup(lxml, &quot;lxml&quot;) urllist = soup.find_all(&apos;a&apos;,href=re.compile(r&apos;^http://weibo.cn/mblog/oripic&apos;,re.I)) first = 0 for imgurl in urllist: urllist_set.add(requests.get(imgurl[&apos;href&apos;], cookies = cookie).url) image_count +=1fo = open(&quot;/Users/Personals/%s&quot;%user_id, &quot;wb&quot;)fo.write(result)word_path=os.getcwd()+&apos;/%d&apos;%user_idprint u&apos;文字微博爬取完毕&apos;link = &quot;&quot;fo2 = open(&quot;/Users/Personals/%s_imageurls&quot;%user_id, &quot;wb&quot;)for eachlink in urllist_set: link = link + eachlink +&quot;\n&quot;fo2.write(link)print u&apos;图片链接爬取完毕&apos;if not urllist_set: print u&apos;该页面中不存在图片&apos;else: #下载图片,保存在当前目录的pythonimg文件夹下 image_path=os.getcwd()+&apos;/weibo_image&apos; if os.path.exists(image_path) is False: os.mkdir(image_path) x=1 for imgurl in urllist_set: temp= image_path + &apos;/%s.jpg&apos; % x print u&apos;正在下载第%s张图片&apos; % x try: urllib.urlretrieve(urllib2.urlopen(imgurl).geturl(),temp) except: print u&quot;该图片下载失败:%s&quot;%imgurl x+=1print u&apos;原创微博爬取完毕，共%d条，保存路径%s&apos;%(word_count-4,word_path)print u&apos;微博图片爬取完毕，共%d张，保存路径%s&apos;%(image_count-1,image_path) 可以将上面的代码保存为wb.py然后在cmd里面运行1python wb.py 大功告成 2016/12/29 星期四 3:16:40 补充：python模块的安装可以用easy_install的命令安装 例如1easy_install lxml python模块pip安装：http://jingyan.baidu.com/article/e73e26c0d94e0524adb6a7ff.html用Python写一个简单的微博爬虫: http://www.jianshu.com/p/7c5a4d7545caMicrosoft Visual C++ Compiler for Python 2.7:http://www.microsoft.com/en-us/download/details.aspx?id=44266]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
